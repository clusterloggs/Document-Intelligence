{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-formrecognizer --quiet\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_INTEL_ENDPOINT = \"https://<your-docint-name>.cognitiveservices.azure.com/\"\n",
    "DOC_INTEL_KEY = \"<your-key>\"\n",
    "\n",
    "client = DocumentAnalysisClient(\n",
    "    endpoint=DOC_INTEL_ENDPOINT,\n",
    "    credential=AzureKeyCredential(DOC_INTEL_KEY)\n",
    ")\n",
    "\n",
    "INPUT_FOLDER = Path(\"/lakehouse/default/Files/incoming_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94808b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper doc classifier\n",
    "\n",
    "def classify_document(file_path):\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        poller = client.begin_analyze_document(\n",
    "            model_id=\"prebuilt-read\",\n",
    "            document=f\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    text = \"\"\n",
    "    for page in result.pages:\n",
    "        for line in page.lines:\n",
    "            text += line.content + \" \"\n",
    "\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    if \"form 1040\" in text_lower:\n",
    "        return \"1040\", \"prebuilt-tax.us.1040\"\n",
    "\n",
    "    if \"1099\" in text_lower:\n",
    "        return \"1099\", \"prebuilt-tax.us.1099\"\n",
    "\n",
    "    if \"driver license\" in text_lower or \"driver's license\" in text_lower:\n",
    "        return \"license\", \"prebuilt-idDocument\"\n",
    "\n",
    "    return \"unknown\", \"prebuilt-read\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccabc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction loop\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in INPUT_FOLDER.iterdir():\n",
    "\n",
    "    if file.suffix.lower() not in [\".png\", \".jpg\", \".jpeg\", \".pdf\"]:\n",
    "        continue\n",
    "\n",
    "    document_id = file.stem\n",
    "\n",
    "    print(f\"Processing: {file.name}\")\n",
    "\n",
    "    \n",
    "    doc_type, model_id = classify_document(file)\n",
    "\n",
    "    print(f\"Detected type: {doc_type} -> {model_id}\")\n",
    "\n",
    "    \n",
    "    with open(file, \"rb\") as f:\n",
    "        poller = client.begin_analyze_document(\n",
    "            model_id=model_id,\n",
    "            document=f\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    \n",
    "    for doc in result.documents:\n",
    "\n",
    "        for field_name, field in doc.fields.items():\n",
    "\n",
    "            value = None\n",
    "            if field.value is not None:\n",
    "                value = str(field.value)\n",
    "            else:\n",
    "                value = field.content\n",
    "\n",
    "            records.append({\n",
    "                \"document_id\": document_id,\n",
    "                \"document_type\": doc_type,\n",
    "                \"field_name\": field_name,\n",
    "                \"field_value\": value,\n",
    "                \"field_type\": str(field.value_type),\n",
    "                \"confidence_score\": float(field.confidence),\n",
    "                \"extracted_at\": datetime.utcnow().isoformat()\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab23fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(records)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"document_fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c58ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log table\n",
    "docs = df.select(\"document_id\", \"document_type\").distinct()\n",
    "docs.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"document_registry\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "Python",
   "name": "jupyter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
